{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORtPCUAgOL3dbr0Xatr0cB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeerHamza-345/Q2/blob/main/Project_01_LangChainand_and_Gemini_Flash_2_0_Integration_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:  \n",
        "Create a Google Colab Notebook using LangChain and the Google Gemini Flash 2.0 API.  \n",
        "Follow these steps:  \n",
        "          1. Set up your Colab environment by installing the required libraries and configuring the Gemini Flash model.  \n",
        "          2. Define a prompt template and create an LLM chain using LangChain.  \n",
        "          3. Run the chain with at least three user-defined questions and display the model's responses.  \n",
        "          4. Experiment with different prompt templates, parameters (e.g., temperature), and chain configurations to optimize responses.  \n"
      ],
      "metadata": {
        "id": "W62OveKRhBe3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LilpAYTKduP2",
        "outputId": "4de99d91-6ba9-48b9-f05f-218d14d8c322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m924.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --q  langchain langchain-google-genai # required libaries installed\n",
        "# 1. Set up your Colab environment by installing the required libraries and configuring the Gemini Flash model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_google_genai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "A6Hheno5fWIM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "LANGCHAIN_API_KEY= userdata.get('LANGCHAIN_API_KEY')\n",
        "#LANGCHAIN_TRACING_V2=true\n",
        "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "\n",
        "LANGCHAIN_PROJECT=\"Project_01_LangChain\"\n"
      ],
      "metadata": {
        "id": "TtfDBoJefWE5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_2_0:ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "  model = \"gemini-1.5-flash\",\n",
        "  api_key = GOOGLE_API_KEY\n",
        ")\n",
        "#Audio, images, videos, and text\tText, images (coming soon), and audio (coming soon)\n",
        "#Next generation features, speed, and multimodal generation for a diverse variety of tasks"
      ],
      "metadata": {
        "id": "_q8BmtwEfWAq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template and create an LLM chain using LangChain.\n",
        "response = model_2_0.invoke(\"Hello\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwZ42XacfV9f",
        "outputId": "3bf087d3-152a-4f55-bb9a-54fcf91bf8d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello there! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.llms import GoogleGeminiFlash\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Step 1: Configure the Gemini Flash model\n",
        "llm= model_2_0\n",
        "\n",
        "# Step 2: Create a Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Step 3: Create the LLM Chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Step 4: Run the Chain with a Sample Question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "# Print the Response\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFEcigxkjoPb",
        "outputId": "aa393492-052a-4120-8a7e-25ec281b07c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is LangChain?\n",
            "Answer: LangChain is a framework for developing applications powered by large language models (LLMs).  It simplifies the process of building these applications by providing tools and components for:\n",
            "\n",
            "* **Connecting to LLMs:** LangChain allows you to easily interact with various LLMs, including OpenAI, Hugging Face Hub models, and others, without needing to worry about the specific API details.\n",
            "\n",
            "* **Managing memory:**  LLMs are stateless, meaning they don't remember past interactions. LangChain offers memory mechanisms to allow your applications to maintain context across multiple interactions with the LLM, leading to more coherent and useful conversations.\n",
            "\n",
            "* **Chain building:** It provides tools to chain together multiple LLMs or other components (e.g., prompts, document loaders) to create more complex applications. This allows you to build applications that perform multiple tasks in sequence.\n",
            "\n",
            "* **Agents:** LangChain supports agents that can decide which LLMs or tools to use based on the user's request.  This enables more flexible and adaptable applications that can handle a wider range of tasks.\n",
            "\n",
            "* **Index building:**  LangChain provides tools to index and query your own documents, allowing you to build applications that leverage your private data with LLMs.\n",
            "\n",
            "In essence, LangChain acts as a Lego set for building LLM-powered applications, providing pre-built blocks and connectors that make development faster and easier.  It abstracts away much of the complexity associated with interacting with LLMs, allowing developers to focus on the application logic.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}