{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqBvOdcPkhD5FYdkHJOfJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeerHamza-345/Q2/blob/main/Project_01_LangChainand_and_Gemini_Flash_2_0_Integration_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:  \n",
        "Create a Google Colab Notebook using LangChain and the Google Gemini Flash 2.0 API.  \n",
        "Follow these steps:  \n",
        "          1. Set up your Colab environment by installing the required libraries and configuring the Gemini Flash model.  \n",
        "          2. Define a prompt template and create an LLM chain using LangChain.  \n",
        "          3. Run the chain with at least three user-defined questions and display the model's responses.  \n",
        "          4. Experiment with different prompt templates, parameters (e.g., temperature), and chain configurations to optimize responses.  \n"
      ],
      "metadata": {
        "id": "W62OveKRhBe3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LilpAYTKduP2",
        "outputId": "a1084251-7c9d-479f-d226-99799dc27dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --q  langchain langchain-google-genai # required libaries installed\n",
        "# 1. Set up your Colab environment by installing the required libraries and configuring the Gemini Flash model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_google_genai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "A6Hheno5fWIM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "LANGCHAIN_API_KEY= userdata.get('LANGCHAIN_API_KEY')\n",
        "#LANGCHAIN_TRACING_V2=true\n",
        "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "\n",
        "LANGCHAIN_PROJECT=\"Project_01_LangChain\"\n"
      ],
      "metadata": {
        "id": "TtfDBoJefWE5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_2_0:ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "  model = \"gemini-2.0-flash-exp\",\n",
        "  api_key = GOOGLE_API_KEY\n",
        ")\n",
        "#Audio, images, videos, and text\tText, images (coming soon), and audio (coming soon)\n",
        "#Next generation features, speed, and multimodal generation for a diverse variety of tasks"
      ],
      "metadata": {
        "id": "_q8BmtwEfWAq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template and create an LLM chain using LangChain.\n",
        "response = model_2_0.invoke(\"Hello\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwZ42XacfV9f",
        "outputId": "f193ec54-2317-438f-c7c8-bccfef9d5117"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.llms import GoogleGeminiFlash\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Step 1: Configure the Gemini Flash model\n",
        "llm= model_2_0\n",
        "\n",
        "# Step 2: Create a Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Step 3: Create the LLM Chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Step 4: Run the Chain with a Sample Question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "# Print the Response\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFEcigxkjoPb",
        "outputId": "d71c1016-9f22-419c-9919-9af1771a72e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fddde645e04c>:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
            "<ipython-input-6-fddde645e04c>:19: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\": question})\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is LangChain?\n",
            "Answer: LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolkit that provides building blocks and abstractions to make working with LLMs easier, more flexible, and more powerful. \n",
            "\n",
            "Here's a breakdown of what that means:\n",
            "\n",
            "**Key Concepts of LangChain:**\n",
            "\n",
            "* **Chains:** LangChain allows you to create \"chains\" of operations, where the output of one component becomes the input of the next. This lets you build complex workflows by combining different LLM functionalities and external tools.\n",
            "* **Components:** LangChain provides a variety of pre-built components, including:\n",
            "    * **LLMs (Language Models):** Integrations with various LLM providers like OpenAI, Hugging Face, and Google.\n",
            "    * **Prompts:** Tools for creating and managing prompts to guide LLMs.\n",
            "    * **Indexes:** Methods for indexing and retrieving information from data sources, making it easier to use external knowledge with LLMs.\n",
            "    * **Memory:** Mechanisms for managing conversation history and context.\n",
            "    * **Agents:** Frameworks for allowing LLMs to interact with the environment and tools to perform actions.\n",
            "    * **Callbacks:** Methods for tracking and logging the progress of chains and agents.\n",
            "* **Flexibility and Modularity:** LangChain promotes a modular design, allowing you to mix and match components to create custom solutions. You can easily swap out LLMs, data sources, or other components without rewriting significant code.\n",
            "* **Abstraction:** LangChain abstracts away some of the complexities of working directly with LLM APIs, making it easier for developers to focus on building applications.\n",
            "* **Community Driven:** Being an open-source project, LangChain benefits from a large and active community, which contributes to its development, provides support, and shares knowledge.\n",
            "\n",
            "**What Problems Does LangChain Solve?**\n",
            "\n",
            "* **Simplified LLM Integration:** It simplifies the process of connecting to and using different LLMs.\n",
            "* **Complex Workflow Management:** It makes it easier to design and manage complex sequences of operations involving LLMs.\n",
            "* **External Data Integration:** It enables seamless integration of external data sources into LLM applications.\n",
            "* **Context Management:** It provides tools for maintaining context in conversations and multi-step workflows.\n",
            "* **Tool Usage:** It allows LLMs to interact with external tools and APIs, expanding their capabilities.\n",
            "* **Rapid Prototyping:** It speeds up the development process by offering pre-built components and abstractions.\n",
            "\n",
            "**In essence, LangChain is about making it easier to build powerful and sophisticated applications using large language models by providing a structured and flexible framework.** It allows developers to focus on the logic and functionality of their applications rather than the low-level details of interacting with LLMs.\n",
            "\n",
            "**Think of it like this:** If you wanted to build a house, you could gather all the raw materials and build everything from scratch. Or, you could use a set of pre-made tools and components from a building supply store, which would greatly simplify the process. LangChain is like that building supply store for LLM applications.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}